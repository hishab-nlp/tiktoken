{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk_nahin/token/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from _tiktoken import TikTokenTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def llama3_tokenizer(path: str) -> TikTokenTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer for Llama3.\n",
    "\n",
    "    Args:\n",
    "        path (str): path to the tokenizer\n",
    "\n",
    "    Returns:\n",
    "        TikTokenTokenizer: Instantiation of the Llama3 tokenizer\n",
    "    \"\"\"\n",
    "    tiktoken = TikTokenTokenizer(path)\n",
    "    tiktoken.pad_id = 0\n",
    "    return tiktoken\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1=llama3_tokenizer(\"/home/sk_nahin/token/llama_tokenizer/tokenizer.model\")\n",
    "tokenizer_2=llama3_tokenizer(\"/home/sk_nahin/token/tokenizer_llama_plus_48K/tokenizer.model\")\n",
    "tokenizer_3=llama3_tokenizer(\"/home/sk_nahin/token/tokenizer_llama_plus_62K/tokenizer.model\")\n",
    "tokenizer_4=llama3_tokenizer(\"/home/sk_nahin/token/tokenizer_llama_plus_32K/tokenizer.model\")\n",
    "tokenizer_5=llama3_tokenizer(\"/home/sk_nahin/token/tokenizer_llama_plus_80K/tokenizer.model\")\n",
    "tokenizer_6=llama3_tokenizer(\"/home/sk_nahin/token/tokenizer_llama_plus_96K/tokenizer.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "gemma = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"সরকারি চাকরিতে কোটাব্যবস্থা বাতিল করে ২০১৮ সালে সরকারের জারি করা পরিপত্র পুনর্বহালসহ কয়েকটি দাবিতে শিক্ষার্থী ও চাকরিপ্রত্যাশীরা আন্দোলন করছেন। এই আন্দোলন এখন দেশের বিভিন্ন এলাকায় ছড়িয়ে পড়ছে। এমন পরিপ্রেক্ষিতে খোঁজ নিয়ে জানা গেল, সরকারি চাকরিতে কোটাপদ্ধতি চালুর ইতিহাসটি বেশ দীর্ঘ। স্বাধীনতার পর থেকেই বিভিন্ন শ্রেণির চাকরিতে কোটাব্যবস্থা চলে আসছিল। একপর্যায়ে কোটা সংস্কারের দাবিতে শিক্ষার্থী ও চাকরিপ্রার্থীদের আন্দোলনের মুখে ২০১৮ সালের অক্টোবরে নবম থেকে ১৩তম গ্রেডের (প্রথম ও দ্বিতীয় শ্রেণি) সরকারি চাকরিতে কোটা বাতিল করে পরিপত্র জারি করেছিল জনপ্রশাসন মন্ত্রণালয়।\"\n",
    "sen = \"রংপুরে বেগম রোকেয়া বিশ্ববিদ্যালয়ের শিক্ষার্থী আবু সাঈদের বুক ও পেট পুলিশের গুলিতে ঝাঁঝরা হয়ে গিয়েছিল। তাঁর গলা থেকে ঊরু পর্যন্ত ছিল ছররা গুলির আঘাত। অথচ পুলিশের করা মামলার প্রাথমিক তথ্য বিবরণীতে (এফআইআর) বলা হয়েছে, আন্দোলনকারীদের ছোড়া গুলি ও ইটপাটকেল নিক্ষেপের এক পর্যায়ে আবু সাঈদের মৃত্যু হয়।\"\n",
    "# pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words:  46\n",
      "llama:  364 tokens:  ['র', '�', '�', '�', '�', '�', '�', 'র', 'ে', ' �', '�', 'ে', '�', '�', '�', '�', ' �', '�', '�', '�', '�', '�', 'ে', '�', '�', 'া', ' �', '�', 'ি�', '�', '্�', '�', '�', '�', 'ি�', '�', '্�', '�', 'া�', '�', '�', '�', 'ে', 'র', ' �', '�', 'ি�', '�', '্�', '�', 'া�', '�', '্�', '�', '�', '�', ' �', '�', '�', '�', '�', '�', ' �', '�', 'া�', '�', '�', '�', 'ে', 'র', ' �', '�', '�', '�', '�', '�', ' �', '�', ' �', '�', 'ে', '�', '�', ' �', '�', '�', '�', '�', '�', 'ি�', '�', 'ে', 'র', ' �', '�', '�', '�', '�', '�', 'ি�', '�', 'ে', ' �', '�', 'া�', '�', '�', '�', 'র', 'া', ' �', '�', '�', '�', 'ে', ' �', '�', 'ি', '�', '�', 'ে', '�', '�', 'ি�', '�', '।', ' �', '�', 'া�', '�', 'র', ' �', '�', '�', '�', 'া', ' �', '�', 'ে', '�', '�', 'ে', ' �', '�', 'র', '�', '�', ' �', '�', 'র', '্�', '�', 'ন', '্�', '�', ' �', '�', 'ি�', '�', ' �', '�', 'র', 'র', 'া', ' �', '�', '�', '�', '�', '�', 'ি', 'র', ' �', '�', '�', '�', 'া�', '�', '।', ' �', '�', '�', '�', '�', '�', ' �', '�', '�', '�', '�', '�', 'ি�', '�', 'ে', 'র', ' �', '�', 'র', 'া', ' �', '�', 'া�', '�', '�', '�', 'া�', '�', ' �', '�', '্�', '�', 'া�', '�', '�', '�', 'ি�', '�', ' �', '�', '�', '�', '্�', '�', ' �', '�', 'ি�', '�', 'র', '�', '�', '�', '�', '�', '�', 'ে', ' (', '�', '�', '�', '�', '�', '�', '�', '�', '�', '�', 'র', ')', ' �', '�', '�', '�', 'া', ' �', '�', '�', '�', 'ে', '�', '�', 'ে', ',', ' �', '�', 'ন', '্�', '�', '�', '�', '�', '�', 'ন', '�', '�', 'া�', '�', '�', '�', '�', '�', 'ে', 'র', ' �', '�', '�', '�', '�', '�', 'া', ' �', '�', '�', '�', '�', '�', 'ি', ' �', '�', ' �', '�', '�', '�', '�', '�', 'া�', '�', '�', '�', 'ে', '�', '�', ' �', '�', 'ি�', '�', '্�', '�', 'ে', '�', '�', 'ে', 'র', ' �', '�', '�', '�', ' �', '�', 'র', '্�', '�', 'া', '�', '�', 'ে', ' �', '�', '�', '�', '�', '�', ' �', '�', 'া�', '�', '�', '�', 'ে', 'র', ' �', '�', '�', '�', '�', '�', '্�', '�', '�', '�', ' �', '�', '�', '�', '।']\n",
      "llama+32K:  106 tokens:  ['র', 'ং�', '�ুরে', ' বে�', '��', '�', ' রো�', '�ে', 'য়া', ' বিশ্', 'ব�', '��', 'ি�', '�্য', 'া�', '��', '�ের', ' শিক্ষার্থী', ' আব�', '�', ' সাঈ�', '�ের', ' বু�', '�', ' ও', ' পেট', ' পুল', 'িশের', ' গুলি�', '�ে', ' ঝাঁ�', '�', 'রা', ' হ�', '�ে', ' গ', 'ি', 'য়েছিল', '।', ' তাঁর', ' গলা', ' থেকে', ' ঊ', 'রু', ' পর্যন্ত', ' ছিল', ' ছ', 'র', 'রা', ' গুল', 'ির', ' আঘাত', '।', ' অথ', 'চ', ' পুল', 'িশের', ' করা', ' মাম�', '�ার', ' প্রাথমিক', ' তথ্য', ' বি�', '��', 'র', 'ণীতে', ' (', 'এফ�', '�ইআর', ')', ' বলা', ' হ�', '�েছে', ',', ' আন', '্�', '��', '��', '�ন�', '�ারীদের', ' ছো', 'ড়া', ' গুল', 'ি', ' ও', ' ইট', 'পাট�', '�েল', ' নিক', '্�', '�ে', 'পের', ' এক', ' পর্যা', 'য়ে', ' আব�', '�', ' সাঈ�', '�ের', ' মৃত', '্�', '��', '�', ' হ�', '�', '।']\n",
      "llama+48K:  91 tokens:  ['র', 'ং�', '�ুরে', ' বেগম', ' রো�', '�ে', 'য়া', ' বিশ্ববি�', '�্য', 'া�', '��', '�ের', ' শিক্ষার্থী', ' আব�', '�', ' সাঈ�', '�ের', ' বুক', ' ও', ' পেট', ' পুলিশের', ' গুলিতে', ' ঝাঁ�', '�', 'রা', ' হ�', '�ে', ' গি', 'য়েছিল', '।', ' তাঁর', ' গলা', ' থেকে', ' ঊর', 'ু', ' পর্যন্ত', ' ছিল', ' ছ', 'র', 'রা', ' গুলি', 'র', ' আঘাত', '।', ' অথ', 'চ', ' পুলিশের', ' করা', ' মামলার', ' প্রাথমিক', ' তথ্য', ' বি�', '��', 'র', 'ণীতে', ' (', 'এফ�', '�ইআর', ')', ' বলা', ' হ�', '�েছে', ',', ' আন', '্�', '��', '��', '�ন�', '�ারীদের', ' ছো', 'ড়া', ' গুলি', ' ও', ' ইট', 'পাট�', '�েল', ' নিক', '্�', '�ে', 'পের', ' এক', ' পর্যা', 'য়ে', ' আব�', '�', ' সাঈ�', '�ের', ' মৃত্যু', ' হ�', '�', '।']\n",
      "llama+62K:  87 tokens:  ['র', 'ং�', '�ুরে', ' বেগম', ' রো�', '�ে', 'য়া', ' বিশ্ববি�', '�্য', 'া�', '��', '�ের', ' শিক্ষার্থী', ' আবু', ' সাঈ�', '�ের', ' বুক', ' ও', ' পেট', ' পুলিশের', ' গুলিতে', ' ঝাঁ�', '�', 'রা', ' হয়ে', ' গি', 'য়েছিল', '।', ' তাঁর', ' গলা', ' থেকে', ' ঊর', 'ু', ' পর্যন্ত', ' ছিল', ' ছ', 'ররা', ' গুলি', 'র', ' আঘাত', '।', ' অথ', 'চ', ' পুলিশের', ' করা', ' মামলার', ' প্রাথমিক', ' তথ্য', ' বি�', '��', 'র', 'ণীতে', ' (', 'এফ�', '�ইআর', ')', ' বলা', ' হয়ে', 'ছে', ',', ' আন', '্�', '��', '��', '�ন�', '�ারীদের', ' ছো', 'ড়া', ' গুলি', ' ও', ' ইট', 'পাট�', '�েল', ' নিক', '্�', '�ে', 'পের', ' এক', ' পর্যা', 'য়ে', ' আবু', ' সাঈ�', '�ের', ' মৃত্যু', ' হ�', '�', '।']\n",
      "llama+80K:  85 tokens:  ['র', 'ং�', '�ুরে', ' বেগম', ' রো�', '�ে', 'য়া', ' বিশ্ববি�', '�্য', 'া�', '��', '�ের', ' শিক্ষার্থী', ' আবু', ' সাঈ�', '�ের', ' বুক', ' ও', ' পেট', ' পুলিশের', ' গুলিতে', ' ঝাঁ�', '�', 'রা', ' হয়ে', ' গি', 'য়েছিল', '।', ' তাঁর', ' গলা', ' থেকে', ' ঊর', 'ু', ' পর্যন্ত', ' ছিল', ' ছ', 'ররা', ' গুলির', ' আঘাত', '।', ' অথ', 'চ', ' পুলিশের', ' করা', ' মামলার', ' প্রাথমিক', ' তথ্য', ' বি�', '��', 'র', 'ণীতে', ' (', 'এফ�', '�ইআর', ')', ' বলা', ' হয়ে', 'ছে', ',', ' আন', '্�', '��', '��', '�ন�', '�ারীদের', ' ছো', 'ড়া', ' গুলি', ' ও', ' ইট', 'পাট�', '�েল', ' নিক্�', '�ে', 'পের', ' এক', ' পর্যা', 'য়ে', ' আবু', ' সাঈ�', '�ের', ' মৃত্যু', ' হ�', '�', '।']\n",
      "llama+96K:  83 tokens:  ['র', 'ং�', '�ুরে', ' বেগম', ' রো�', '�ে', 'য়া', ' বিশ্ববি�', '�্য', 'া�', '��', '�ের', ' শিক্ষার্থী', ' আবু', ' সাঈ�', '�ের', ' বুক', ' ও', ' পেট', ' পুলিশের', ' গুলিতে', ' ঝাঁ�', '�', 'রা', ' হয়ে', ' গি', 'য়েছিল', '।', ' তাঁর', ' গলা', ' থেকে', ' ঊর', 'ু', ' পর্যন্ত', ' ছিল', ' ছ', 'ররা', ' গুলির', ' আঘাত', '।', ' অথ', 'চ', ' পুলিশের', ' করা', ' মামলার', ' প্রাথমিক', ' তথ্য', ' বি�', '��', 'র', 'ণীতে', ' (', 'এফ�', '�ইআর', ')', ' বলা', ' হয়ে', 'ছে', ',', ' আন', '্�', '��', '��', '�ন�', '�ারীদের', ' ছো', 'ড়া', ' গুলি', ' ও', ' ইট', 'পাট�', '�েল', ' নিক্ষেপের', ' এক', ' পর্যা', 'য়ে', ' আবু', ' সাঈ�', '�ের', ' মৃত্যু', ' হ�', '�', '।']\n",
      "gemma:  175 tokens:  ['র', 'ং', 'প', 'ুর', 'ে', '▁ব', 'ে', 'গ', 'ম', '▁র', 'ো', 'কে', 'য়', 'া', '▁বি', 'শ', '্ব', 'বি', 'দ', '্য', 'াল', 'য়', 'ের', '▁শ', 'িক', '্ষ', 'ার্', 'থ', 'ী', '▁আ', 'ব', 'ু', '▁সা', 'ঈ', 'দের', '▁ব', 'ু', 'ক', '▁ও', '▁প', 'ে', 'ট', '▁প', 'ু', 'লি', 'শ', 'ের', '▁গ', 'ুল', 'িতে', '▁', 'ঝ', 'াঁ', 'ঝ', 'রা', '▁হ', 'য়ে', '▁গ', 'ি', 'য়ে', 'ছ', 'িল', '।', '▁ত', 'াঁ', 'র', '▁গ', 'লা', '▁থেকে', '▁', 'ঊ', 'র', 'ু', '▁পর', '্য', 'ন্ত', '▁ছ', 'িল', '▁ছ', 'র', 'রা', '▁গ', 'ু', 'লি', 'র', '▁আ', 'ঘ', 'াত', '।', '▁অ', 'থ', 'চ', '▁প', 'ু', 'লি', 'শ', 'ের', '▁করা', '▁ম', 'াম', 'ল', 'ার', '▁প্র', 'া', 'থ', 'ম', 'িক', '▁ত', 'থ্য', '▁বি', 'ব', 'রণ', 'ী', 'তে', '▁(', 'এ', 'ফ', 'আ', 'ই', 'আ', 'র', ')', '▁ব', 'লা', '▁হ', 'য়ে', 'ছে', ',', '▁আ', 'ন্দ', 'ো', 'ল', 'ন', 'কার', 'ী', 'দের', '▁ছ', 'ো', 'ড়', 'া', '▁গ', 'ু', 'লি', '▁ও', '▁ই', 'ট', 'প', 'া', 'ট', 'কে', 'ল', '▁ন', 'িক', '্ষ', 'ে', 'প', 'ের', '▁এক', '▁পর', '্যা', 'য়ে', '▁আ', 'ব', 'ু', '▁সা', 'ঈ', 'দের', '▁ম', 'ৃ', 'ত', '্য', 'ু', '▁হ', 'য়', '।']\n"
     ]
    }
   ],
   "source": [
    "print(\"total words: \",len(sen.split()))\n",
    "\n",
    "tokens_1 = [tokenizer_1.decode([i]) for i in tokenizer_1.encode(sen,add_bos=False,add_eos=False)]\n",
    "print(\"llama: \", len(tokens_1), \"tokens: \", tokens_1)\n",
    "tokens_4 = [tokenizer_4.decode([i]) for i in tokenizer_4.encode(sen,add_bos=False,add_eos=False)]\n",
    "print(\"llama+32K: \", len(tokens_4), \"tokens: \", tokens_4)\n",
    "tokens_2 = [tokenizer_2.decode([i]) for i in tokenizer_2.encode(sen,add_bos=False,add_eos=False)]\n",
    "print(\"llama+48K: \", len(tokens_2), \"tokens: \", tokens_2)\n",
    "tokens_3 = [tokenizer_3.decode([i]) for i in tokenizer_3.encode(sen,add_bos=False,add_eos=False)]\n",
    "print(\"llama+62K: \", len(tokens_3), \"tokens: \", tokens_3)\n",
    "\n",
    "tokens_5 = [tokenizer_5.decode([i]) for i in tokenizer_5.encode(sen,add_bos=False,add_eos=False)]\n",
    "print(\"llama+80K: \", len(tokens_5), \"tokens: \", tokens_5)\n",
    "\n",
    "tokens_6 = [tokenizer_6.decode([i]) for i in tokenizer_6.encode(sen,add_bos=False,add_eos=False)]\n",
    "print(\"llama+96K: \", len(tokens_6), \"tokens: \", tokens_6)\n",
    "\n",
    "\n",
    "tokens_gemma = gemma.tokenize(sen)\n",
    "print(\"gemma: \", len(tokens_gemma), \"tokens: \", tokens_gemma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 37.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def load_text(files):\n",
    "    texts = \"\"\n",
    "    for file in tqdm(files):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            texts += text + '\\n'\n",
    "    return texts\n",
    "\n",
    "all_files = glob.glob(\"/home/sk_nahin/token/test_data/*\")[:1]\n",
    "\n",
    "all_text = load_text(all_files)[:1000000]\n",
    "\n",
    "total_words = len(all_text.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_words:  152269 \n",
      "\n",
      "-------------token per word--------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama:  7.839717867720942\n",
      "llama+32K:  2.134636728421412\n",
      "llama+48K:  1.902882398912451\n",
      "llama+62K:  1.7945872107914282\n",
      "llama+80K:  1.7369786364919977\n",
      "llama+96K:  1.7034327407417136\n",
      "llama+62K:  3.8186104853909857\n"
     ]
    }
   ],
   "source": [
    "print(\"total_words: \",total_words,\"\\n\")\n",
    "print(\"-------------token per word--------------\\n\")\n",
    "tokens_1 = [tokenizer_1.decode([i]) for i in tokenizer_1.encode(all_text,add_bos=False,add_eos=False)]\n",
    "print(\"llama: \", len(tokens_1)/total_words)\n",
    "tokens_4 = [tokenizer_4.decode([i]) for i in tokenizer_4.encode(all_text,add_bos=False,add_eos=False)]\n",
    "print(\"llama+32K: \", len(tokens_4)/total_words)\n",
    "tokens_2 = [tokenizer_2.decode([i]) for i in tokenizer_2.encode(all_text,add_bos=False,add_eos=False)]\n",
    "print(\"llama+48K: \", len(tokens_2)/total_words)\n",
    "tokens_3 = [tokenizer_3.decode([i]) for i in tokenizer_3.encode(all_text,add_bos=False,add_eos=False)]\n",
    "print(\"llama+62K: \", len(tokens_3)/total_words)\n",
    "\n",
    "tokens_5 = [tokenizer_5.decode([i]) for i in tokenizer_5.encode(all_text,add_bos=False,add_eos=False)]\n",
    "print(\"llama+80K: \", len(tokens_5)/total_words)\n",
    "\n",
    "tokens_6 = [tokenizer_6.decode([i]) for i in tokenizer_6.encode(all_text,add_bos=False,add_eos=False)]\n",
    "print(\"llama+96K: \", len(tokens_6)/total_words)\n",
    "\n",
    "\n",
    "tokens_gemma = gemma.tokenize(all_text)\n",
    "print(\"llama+62K: \", len(tokens_gemma)/total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
